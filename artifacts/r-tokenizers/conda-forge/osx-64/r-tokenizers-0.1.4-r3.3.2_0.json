{
 "about": {
  "channels": [
   "https://conda.anaconda.org/conda-forge/osx-64",
   "https://conda.anaconda.org/conda-forge/noarch",
   "https://repo.continuum.io/pkgs/free/osx-64",
   "https://repo.continuum.io/pkgs/free/noarch",
   "https://repo.continuum.io/pkgs/pro/osx-64",
   "https://repo.continuum.io/pkgs/pro/noarch"
  ],
  "conda_build_version": "2.1.10",
  "conda_env_version": "4.2.13",
  "conda_private": false,
  "conda_version": "4.2.13",
  "env_vars": {
   "CIO_TEST": "<not set>",
   "CONDA_DEFAULT_ENV": "root",
   "CONDA_ENVS_PATH": "<not set>",
   "DYLD_LIBRARY_PATH": "<not set>",
   "PATH": "/Users/travis/miniconda3/bin:/Users/travis/.rvm/gems/ruby-2.0.0-p643/bin:/Users/travis/.rvm/gems/ruby-2.0.0-p643@global/bin:/Users/travis/.rvm/rubies/ruby-2.0.0-p643/bin:/Users/travis/.rvm/bin:/Users/travis/bin:/Users/travis/.local/bin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin",
   "PYTHONHOME": "<not set>",
   "PYTHONPATH": "<not set>"
  },
  "home": "https://github.com/ropensci/tokenizers",
  "license": "MIT",
  "license_family": "MIT",
  "offline": false,
  "root_pkgs": [
   "anaconda-client-1.6.3-py36_0",
   "conda-forge::beautifulsoup4-4.5.3-py36_0",
   "cffi-1.9.1-py36_0",
   "conda-forge::chardet-3.0.2-py36_1",
   "conda-forge::clyent-1.2.1-py36_0",
   "conda-forge::conda-4.2.13-py36_0",
   "conda-forge::conda-build-2.1.10-py36_0",
   "conda-forge::conda-env-2.6.0-0",
   "conda-forge::conda-forge-build-setup-4.4.4-0",
   "conda-forge::conda-verify-2.0.0-py36_0",
   "cryptography-1.7.1-py36_0",
   "conda-forge::filelock-2.0.6-py36_0",
   "idna-2.2-py36_0",
   "conda-forge::jinja2-2.9.5-py36_0",
   "conda-forge::markupsafe-0.23-py36_1",
   "openssl-1.0.2k-0",
   "pip-9.0.1-py36_1",
   "conda-forge::pkginfo-1.2.1-py36_0",
   "pyasn1-0.1.9-py36_0",
   "pycosat-0.6.1-py36_1",
   "pycparser-2.17-py36_0",
   "pycrypto-2.6.1-py36_4",
   "pyopenssl-16.2.0-py36_0",
   "python-3.6.0-0",
   "conda-forge::python-dateutil-2.6.0-py36_0",
   "conda-forge::pytz-2017.2-py36_0",
   "conda-forge::pyyaml-3.12-py36_1",
   "readline-6.2-2",
   "requests-2.12.4-py36_0",
   "ruamel_yaml-0.11.14-py36_1",
   "setuptools-27.2.0-py36_0",
   "six-1.10.0-py36_0",
   "sqlite-3.13.0-0",
   "tk-8.5.18-0",
   "wheel-0.29.0-py36_0",
   "xz-5.2.2-1",
   "yaml-0.1.6-0",
   "zlib-1.2.8-3"
  ],
  "summary": "Convert natural language text into tokens. The tokenizers have a consistent interface and are compatible with Unicode, thanks to being built on the 'stringi' package. Includes tokenizers for shingled n-grams, skip n-grams, words, word stems, sentences, paragraphs, characters, lines, and regular expressions."
 },
 "conda_build_config": {},
 "files": [
  "lib/R/library/tokenizers/INDEX",
  "lib/R/library/tokenizers/Meta/hsearch.rds",
  "lib/R/library/tokenizers/doc/introduction-to-tokenizers.R",
  "lib/R/library/tokenizers/help/tokenizers.rdx",
  "lib/R/library/tokenizers/DESCRIPTION",
  "lib/R/library/tokenizers/doc/introduction-to-tokenizers.html",
  "lib/R/library/tokenizers/libs/tokenizers.so",
  "lib/R/library/tokenizers/html/00Index.html",
  "lib/R/library/tokenizers/NAMESPACE",
  "lib/R/library/tokenizers/Meta/Rd.rds",
  "lib/R/library/tokenizers/Meta/vignette.rds",
  "lib/R/library/tokenizers/NEWS.md",
  "lib/R/library/tokenizers/help/aliases.rds",
  "lib/R/library/tokenizers/html/R.css",
  "lib/R/library/tokenizers/R/tokenizers.rdb",
  "lib/R/library/tokenizers/Meta/nsInfo.rds",
  "lib/R/library/tokenizers/doc/index.html",
  "lib/R/library/tokenizers/doc/introduction-to-tokenizers.Rmd",
  "lib/R/library/tokenizers/R/sysdata.rdx",
  "lib/R/library/tokenizers/R/sysdata.rdb",
  "lib/R/library/tokenizers/help/paths.rds",
  "lib/R/library/tokenizers/Meta/links.rds",
  "lib/R/library/tokenizers/help/tokenizers.rdb",
  "lib/R/library/tokenizers/LICENSE",
  "lib/R/library/tokenizers/R/tokenizers.rdx",
  "lib/R/library/tokenizers/help/AnIndex",
  "lib/R/library/tokenizers/Meta/package.rds",
  "lib/R/library/tokenizers/R/tokenizers"
 ],
 "index": {
  "arch": "x86_64",
  "build": "r3.3.2_0",
  "build_number": 0,
  "depends": [
   "r-base 3.3.2*",
   "r-rcpp >=0.12.3",
   "r-snowballc >=0.5.1",
   "r-stringi >=1.0.1"
  ],
  "license": "MIT",
  "license_family": "MIT",
  "name": "r-tokenizers",
  "platform": "osx",
  "subdir": "osx-64",
  "version": "0.1.4"
 },
 "metadata_version": 1,
 "name": "r-tokenizers",
 "raw_recipe": "{% set version = '0.1.4' %}\n\n{% set posix = 'm2-' if win else '' %}\n{% set native = 'm2w64-' if win else '' %}\n\npackage:\n  name: r-tokenizers\n  version: {{ version|replace(\"-\", \"_\") }}\n\nsource:\n  fn: tokenizers_{{ version }}.tar.gz\n  url:\n    - https://cran.r-project.org/src/contrib/tokenizers_{{ version }}.tar.gz\n    - https://cran.r-project.org/src/contrib/Archive/tokenizers/tokenizers_{{ version }}.tar.gz\n  sha256: 693e19e32605f4c66a51b1eb84258a973ac2fe1d2f919b28e66944721aab9ae1\n\nbuild:\n  number: 0\n  skip: true  # [win32]\n\n  rpaths:\n    - lib/R/lib/\n    - lib/\n\nrequirements:\n  build:\n    - r-base\n    - r-rcpp >=0.12.3\n    - r-snowballc >=0.5.1\n    - r-stringi >=1.0.1\n    - posix                # [win]\n    - {{native}}toolchain  # [win]\n    - gcc                  # [not win]\n\n  run:\n    - r-base\n    - r-rcpp >=0.12.3\n    - r-snowballc >=0.5.1\n    - r-stringi >=1.0.1\n\ntest:\n  commands:\n    - $R -e \"library('tokenizers')\"  # [not win]\n    - \"\\\"%R%\\\" -e \\\"library('tokenizers')\\\"\"  # [win]\n\nabout:\n  home: https://github.com/ropensci/tokenizers\n  license: MIT\n  summary: Convert natural language text into tokens. The tokenizers have a consistent interface\n    and are compatible with Unicode, thanks to being built on the 'stringi' package.\n    Includes tokenizers for shingled n-grams, skip n-grams, words, word stems, sentences,\n    paragraphs, characters, lines, and regular expressions.\n  license_family: MIT\n\nextra:\n  recipe-maintainers:\n    - johanneskoester\n    - bgruening\n",
 "rendered_recipe": {
  "about": {
   "home": "https://github.com/ropensci/tokenizers",
   "license": "MIT",
   "license_family": "MIT",
   "summary": "Convert natural language text into tokens. The tokenizers have a consistent interface and are compatible with Unicode, thanks to being built on the 'stringi' package. Includes tokenizers for shingled n-grams, skip n-grams, words, word stems, sentences, paragraphs, characters, lines, and regular expressions."
  },
  "build": {
   "noarch": "",
   "noarch_python": false,
   "number": "0",
   "rpaths": [
    "lib/R/lib/",
    "lib/"
   ],
   "string": "r3.3.2_0"
  },
  "extra": {
   "final": true,
   "recipe-maintainers": [
    "johanneskoester",
    "bgruening"
   ]
  },
  "package": {
   "name": "r-tokenizers",
   "version": "0.1.4"
  },
  "requirements": {
   "build": [
    "ca-certificates 2017.1.23 1",
    "cairo 1.14.6 4",
    "cloog 0.18.0 0",
    "curl 7.52.1 0",
    "fontconfig 2.12.1 4",
    "freetype 2.7 1",
    "gcc 4.8.5 8",
    "gettext 0.19.7 1",
    "glib 2.51.4 0",
    "gmp 6.1.2 0",
    "graphite2 1.3.9 0",
    "gsl 2.2.1 1",
    "harfbuzz 1.4.3 0",
    "icu 58.1 1",
    "isl 0.12.2 1",
    "jpeg 9b 0",
    "libffi 3.2.1 3",
    "libgcc 4.8.5 1",
    "libiconv 1.14 4",
    "libpng 1.6.28 0",
    "libtiff 4.0.6 7",
    "libxml2 2.9.4 4",
    "mpc 1.0.3 3",
    "mpfr 3.1.5 0",
    "ncurses 5.9 10",
    "openssl 1.0.2k 0",
    "pango 1.40.4 0",
    "pcre 8.39 0",
    "pixman 0.34.0 0",
    "r-base 3.3.2 5",
    "r-rcpp 0.12.10 r3.3.2_0",
    "r-snowballc 0.5.1 r3.3.2_0",
    "r-stringi 1.1.2 r3.3.2_0",
    "readline 6.2 0",
    "tk 8.5.19 1",
    "xz 5.2.2 0",
    "zlib 1.2.8 3"
   ],
   "run": [
    "r-base",
    "r-rcpp >=0.12.3",
    "r-snowballc >=0.5.1",
    "r-stringi >=1.0.1"
   ]
  },
  "source": {
   "fn": "tokenizers_0.1.4.tar.gz",
   "sha256": "693e19e32605f4c66a51b1eb84258a973ac2fe1d2f919b28e66944721aab9ae1",
   "url": [
    "https://cran.r-project.org/src/contrib/tokenizers_0.1.4.tar.gz",
    "https://cran.r-project.org/src/contrib/Archive/tokenizers/tokenizers_0.1.4.tar.gz"
   ]
  },
  "test": {
   "commands": [
    "$R -e \"library('tokenizers')\""
   ]
  }
 },
 "version": "0.1.4"
}