{
 "about": {
  "channels": [
   "https://conda.anaconda.org/conda-forge/linux-64",
   "https://conda.anaconda.org/conda-forge/noarch",
   "https://repo.continuum.io/pkgs/free/linux-64",
   "https://repo.continuum.io/pkgs/free/noarch",
   "https://repo.continuum.io/pkgs/pro/linux-64",
   "https://repo.continuum.io/pkgs/pro/noarch"
  ],
  "conda_build_version": "2.1.10",
  "conda_env_version": "4.2.13",
  "conda_private": false,
  "conda_version": "4.2.13",
  "env_vars": {
   "CIO_TEST": "<not set>",
   "CONDA_DEFAULT_ENV": "root",
   "CONDA_ENVS_PATH": "<not set>",
   "LD_LIBRARY_PATH": "/opt/rh/devtoolset-2/root/usr/lib64:/opt/rh/devtoolset-2/root/usr/lib",
   "PATH": "/opt/conda/bin:/opt/rh/devtoolset-2/root/usr/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/root/bin",
   "PYTHONHOME": "<not set>",
   "PYTHONPATH": "/opt/rh/devtoolset-2/root/usr/lib64/python2.6/site-packages:/opt/rh/devtoolset-2/root/usr/lib/python2.6/site-packages"
  },
  "home": "https://github.com/ropensci/tokenizers",
  "license": "MIT",
  "license_family": "MIT",
  "offline": false,
  "root_pkgs": [
   "conda-forge::anaconda-client-1.5.1-py35_0",
   "conda-forge::beautifulsoup4-4.5.3-py35_0",
   "conda-forge::ca-certificates-2017.1.23-1",
   "conda-forge::certifi-2017.4.17-py35_0",
   "conda-forge::chardet-3.0.2-py35_1",
   "conda-forge::clyent-1.2.1-py35_0",
   "conda-forge::conda-4.2.13-py35_0",
   "conda-forge::conda-build-2.1.10-py35_0",
   "conda-forge::conda-env-2.6.0-0",
   "conda-forge::conda-forge-build-setup-4.4.4-0",
   "conda-forge::conda-verify-2.0.0-py35_0",
   "conda-forge::curl-7.52.1-0",
   "conda-forge::expat-2.1.0-2",
   "conda-forge::filelock-2.0.6-py35_0",
   "conda-forge::git-2.12.2-4",
   "conda-forge::jinja2-2.9.5-py35_0",
   "conda-forge::libiconv-1.14-4",
   "conda-forge::markupsafe-0.23-py35_1",
   "conda-forge::ncurses-5.9-10",
   "conda-forge::openssl-1.0.2k-0",
   "conda-forge::patchelf-0.9-1",
   "conda-forge::pip-9.0.1-py35_0",
   "conda-forge::pkginfo-1.2.1-py35_0",
   "conda-forge::pycosat-0.6.1-py35_0",
   "conda-forge::pycrypto-2.6.1-py35_0",
   "conda-forge::python-3.5.3-1",
   "conda-forge::python-dateutil-2.6.0-py35_0",
   "conda-forge::pytz-2017.2-py35_0",
   "conda-forge::pyyaml-3.12-py35_1",
   "conda-forge::readline-6.2-0",
   "conda-forge::requests-2.13.0-py35_0",
   "conda-forge::ruamel_yaml-0.11.14-py35_0",
   "conda-forge::setuptools-33.1.1-py35_0",
   "conda-forge::six-1.10.0-py35_1",
   "conda-forge::sqlite-3.13.0-1",
   "conda-forge::tk-8.5.19-1",
   "conda-forge::wheel-0.29.0-py35_0",
   "conda-forge::xz-5.2.2-0",
   "conda-forge::yaml-0.1.6-0",
   "conda-forge::zlib-1.2.11-0"
  ],
  "summary": "Convert natural language text into tokens. The tokenizers have a consistent interface and are compatible with Unicode, thanks to being built on the 'stringi' package. Includes tokenizers for shingled n-grams, skip n-grams, words, word stems, sentences, paragraphs, characters, lines, and regular expressions."
 },
 "conda_build_config": {},
 "files": [
  "lib/R/library/tokenizers/doc/index.html",
  "lib/R/library/tokenizers/INDEX",
  "lib/R/library/tokenizers/html/R.css",
  "lib/R/library/tokenizers/NEWS.md",
  "lib/R/library/tokenizers/R/sysdata.rdb",
  "lib/R/library/tokenizers/libs/tokenizers.so",
  "lib/R/library/tokenizers/Meta/links.rds",
  "lib/R/library/tokenizers/doc/introduction-to-tokenizers.R",
  "lib/R/library/tokenizers/Meta/hsearch.rds",
  "lib/R/library/tokenizers/DESCRIPTION",
  "lib/R/library/tokenizers/help/tokenizers.rdb",
  "lib/R/library/tokenizers/help/tokenizers.rdx",
  "lib/R/library/tokenizers/help/AnIndex",
  "lib/R/library/tokenizers/R/sysdata.rdx",
  "lib/R/library/tokenizers/NAMESPACE",
  "lib/R/library/tokenizers/R/tokenizers.rdx",
  "lib/R/library/tokenizers/doc/introduction-to-tokenizers.Rmd",
  "lib/R/library/tokenizers/help/paths.rds",
  "lib/R/library/tokenizers/help/aliases.rds",
  "lib/R/library/tokenizers/LICENSE",
  "lib/R/library/tokenizers/doc/introduction-to-tokenizers.html",
  "lib/R/library/tokenizers/Meta/vignette.rds",
  "lib/R/library/tokenizers/Meta/package.rds",
  "lib/R/library/tokenizers/R/tokenizers.rdb",
  "lib/R/library/tokenizers/R/tokenizers",
  "lib/R/library/tokenizers/Meta/Rd.rds",
  "lib/R/library/tokenizers/Meta/nsInfo.rds",
  "lib/R/library/tokenizers/html/00Index.html"
 ],
 "index": {
  "arch": "x86_64",
  "build": "r3.3.2_0",
  "build_number": 0,
  "depends": [
   "r-base 3.3.2*",
   "r-rcpp >=0.12.3",
   "r-snowballc >=0.5.1",
   "r-stringi >=1.0.1"
  ],
  "license": "MIT",
  "license_family": "MIT",
  "name": "r-tokenizers",
  "platform": "linux",
  "subdir": "linux-64",
  "version": "0.1.4"
 },
 "metadata_version": 1,
 "name": "r-tokenizers",
 "raw_recipe": "{% set version = '0.1.4' %}\n\n{% set posix = 'm2-' if win else '' %}\n{% set native = 'm2w64-' if win else '' %}\n\npackage:\n  name: r-tokenizers\n  version: {{ version|replace(\"-\", \"_\") }}\n\nsource:\n  fn: tokenizers_{{ version }}.tar.gz\n  url:\n    - https://cran.r-project.org/src/contrib/tokenizers_{{ version }}.tar.gz\n    - https://cran.r-project.org/src/contrib/Archive/tokenizers/tokenizers_{{ version }}.tar.gz\n  sha256: 693e19e32605f4c66a51b1eb84258a973ac2fe1d2f919b28e66944721aab9ae1\n\nbuild:\n  number: 0\n  skip: true  # [win32]\n\n  rpaths:\n    - lib/R/lib/\n    - lib/\n\nrequirements:\n  build:\n    - r-base\n    - r-rcpp >=0.12.3\n    - r-snowballc >=0.5.1\n    - r-stringi >=1.0.1\n    - posix                # [win]\n    - {{native}}toolchain  # [win]\n    - gcc                  # [not win]\n\n  run:\n    - r-base\n    - r-rcpp >=0.12.3\n    - r-snowballc >=0.5.1\n    - r-stringi >=1.0.1\n\ntest:\n  commands:\n    - $R -e \"library('tokenizers')\"  # [not win]\n    - \"\\\"%R%\\\" -e \\\"library('tokenizers')\\\"\"  # [win]\n\nabout:\n  home: https://github.com/ropensci/tokenizers\n  license: MIT\n  summary: Convert natural language text into tokens. The tokenizers have a consistent interface\n    and are compatible with Unicode, thanks to being built on the 'stringi' package.\n    Includes tokenizers for shingled n-grams, skip n-grams, words, word stems, sentences,\n    paragraphs, characters, lines, and regular expressions.\n  license_family: MIT\n\nextra:\n  recipe-maintainers:\n    - johanneskoester\n    - bgruening\n",
 "rendered_recipe": {
  "about": {
   "home": "https://github.com/ropensci/tokenizers",
   "license": "MIT",
   "license_family": "MIT",
   "summary": "Convert natural language text into tokens. The tokenizers have a consistent interface and are compatible with Unicode, thanks to being built on the 'stringi' package. Includes tokenizers for shingled n-grams, skip n-grams, words, word stems, sentences, paragraphs, characters, lines, and regular expressions."
  },
  "build": {
   "noarch": "",
   "noarch_python": false,
   "number": "0",
   "rpaths": [
    "lib/R/lib/",
    "lib/"
   ],
   "string": "r3.3.2_0"
  },
  "extra": {
   "final": true,
   "recipe-maintainers": [
    "johanneskoester",
    "bgruening"
   ]
  },
  "package": {
   "name": "r-tokenizers",
   "version": "0.1.4"
  },
  "requirements": {
   "build": [
    "gcc 4.8.5 7",
    "r-base 3.3.2 5",
    "r-rcpp 0.12.10 r3.3.2_0",
    "libgcc 5.2.0 0",
    "xz 5.2.2 0",
    "jpeg 9b 0",
    "ncurses 5.9 10",
    "tk 8.5.19 1",
    "ca-certificates 2017.1.23 1",
    "cloog 0.18.0 0",
    "harfbuzz 1.4.3 0",
    "bzip2 1.0.6 1",
    "readline 6.2 0",
    "gettext 0.19.7 1",
    "graphite2 1.3.9 0",
    "pixman 0.34.0 0",
    "openssl 1.0.2k 0",
    "pango 1.40.4 0",
    "libiconv 1.14 4",
    "glib 2.51.4 0",
    "r-snowballc 0.5.1 r3.3.2_0",
    "mpfr 3.1.5 0",
    "freetype 2.7 1",
    "curl 7.52.1 0",
    "gsl 2.2.1 1",
    "libffi 3.2.1 3",
    "libxml2 2.9.4 4",
    "libtiff 4.0.6 7",
    "mpc 1.0.3 4",
    "fontconfig 2.12.1 4",
    "libpng 1.6.28 0",
    "pcre 8.39 0",
    "r-stringi 1.1.2 r3.3.2_0",
    "cairo 1.14.6 4",
    "icu 58.1 1",
    "zlib 1.2.8 3",
    "isl 0.12.2 0",
    "gmp 6.1.2 0"
   ],
   "run": [
    "r-base",
    "r-rcpp >=0.12.3",
    "r-snowballc >=0.5.1",
    "r-stringi >=1.0.1"
   ]
  },
  "source": {
   "fn": "tokenizers_0.1.4.tar.gz",
   "sha256": "693e19e32605f4c66a51b1eb84258a973ac2fe1d2f919b28e66944721aab9ae1",
   "url": [
    "https://cran.r-project.org/src/contrib/tokenizers_0.1.4.tar.gz",
    "https://cran.r-project.org/src/contrib/Archive/tokenizers/tokenizers_0.1.4.tar.gz"
   ]
  },
  "test": {
   "commands": [
    "$R -e \"library('tokenizers')\""
   ]
  }
 },
 "version": "0.1.4"
}