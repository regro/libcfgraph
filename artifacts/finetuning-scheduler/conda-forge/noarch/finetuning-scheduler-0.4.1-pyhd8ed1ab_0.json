{
 "about": {
  "channels": [
   "https://conda.anaconda.org/conda-forge"
  ],
  "conda_build_version": "3.25.0",
  "conda_version": "23.3.1",
  "description": "The FinetuningScheduler callback accelerates and enhances foundational model experimentation with flexible fine-tuning\nschedules. Training with the FinetuningScheduler callback is simple and confers a host of benefits:\n\n- it dramatically increases fine-tuning flexibility\n- expedites and facilitates exploration of model tuning dynamics\n- enables marginal performance improvements of finetuned models\n\nFundamentally, the FinetuningScheduler callback enables multi-phase, scheduled fine-tuning of foundational models.\nGradual unfreezing (i.e. thawing) can help maximize foundational model knowledge retention while allowing (typically\nupper layers of) the model to optimally adapt to new tasks during transfer learning.\n\nFinetuningScheduler orchestrates the gradual unfreezing of models via a fine-tuning schedule that is either implicitly\ngenerated (the default) or explicitly provided by the user (more computationally efficient). Fine-tuning phase\ntransitions are driven by FTSEarlyStopping criteria (a multi-phase extension of EarlyStopping), user-specified epoch\ntransitions or a composition of the two (the default mode). A FinetuningScheduler training session completes when the\nfinal phase of the schedule has its stopping criteria met.\n\nDocumentation\n-------------\n- https://finetuning-scheduler.readthedocs.io/en/stable/\n- https://finetuning-scheduler.readthedocs.io/en/latest/\n",
  "dev_url": "https://github.com/speediedan/finetuning-scheduler",
  "doc_url": "https://finetuning-scheduler.readthedocs.io/en/stable/",
  "env_vars": {
   "CIO_TEST": "<not set>"
  },
  "extra": {
   "copy_test_source_files": true,
   "final": true,
   "recipe-maintainers": [
    "speediedan"
   ]
  },
  "home": "https://github.com/speediedan/finetuning-scheduler",
  "identifiers": [],
  "keywords": [],
  "license": "Apache-2.0",
  "license_file": "LICENSE",
  "root_pkgs": [
   "libsqlite 3.42.0 h2797004_0",
   "tzdata 2023c h71feb2d_0",
   "conda-pack 0.7.1 pyhd8ed1ab_0",
   "brotli-python 1.0.9 py310hd8f1fbe_9",
   "toolz 0.12.0 pyhd8ed1ab_0",
   "importlib_resources 6.0.1 pyhd8ed1ab_0",
   "libgomp 13.1.0 he5830b7_0",
   "ruamel.yaml 0.17.32 py310h2372a71_0",
   "libexpat 2.5.0 hcb278e6_1",
   "libdeflate 1.18 h0b41bf4_0",
   "bzip2 1.0.8 h7f98852_4",
   "pycparser 2.21 pyhd8ed1ab_0",
   "patchelf 0.17.2 h58526e2_0",
   "freetype 2.12.1 hca18f0e_1",
   "pygments 2.16.1 pyhd8ed1ab_0",
   "prompt-toolkit 3.0.39 pyha770c72_0",
   "_openmp_mutex 4.5 2_gnu",
   "six 1.16.0 pyh6c4a22f_0",
   "filelock 3.12.2 pyhd8ed1ab_0",
   "libffi 3.4.2 h7f98852_5",
   "libstdcxx-ng 13.1.0 hfd8a6a1_0",
   "tornado 6.3.2 py310h2372a71_0",
   "requests-toolbelt 1.0.0 pyhd8ed1ab_0",
   "anyio 3.7.1 pyhd8ed1ab_0",
   "boltons 23.0.0 pyhd8ed1ab_0",
   "krb5 1.21.1 h659d440_0",
   "git 2.41.0 pl5321h86e50cf_0",
   "libjpeg-turbo 2.1.5.1 h0b41bf4_0",
   "jsonschema 4.19.0 pyhd8ed1ab_1",
   "referencing 0.30.2 pyhd8ed1ab_0",
   "zipp 3.16.2 pyhd8ed1ab_0",
   "jsonpatch 1.32 pyhd8ed1ab_0",
   "anaconda-project 0.11.1 pyhd8ed1ab_0",
   "libmambapy 1.4.2 py310h1428755_0",
   "pip 23.2.1 pyhd8ed1ab_0",
   "conda-build 3.25.0 py310hff52083_0",
   "libxml2 2.11.5 h0d562d8_0",
   "xz 5.2.6 h166bdaf_0",
   "lzo 2.10 h516909a_1000",
   "libssh2 1.11.0 h0841786_0",
   "colorama 0.4.6 pyhd8ed1ab_0",
   "jsonpointer 2.0 py_0",
   "packaging 23.1 pyhd8ed1ab_0",
   "typing-extensions 4.7.1 hd8ed1ab_0",
   "ld_impl_linux-64 2.40 h41732ed_0",
   "tk 8.6.12 h27826a3_0",
   "mdurl 0.1.0 pyhd8ed1ab_0",
   "jupyter_core 5.3.1 py310hff52083_0",
   "exceptiongroup 1.1.2 pyhd8ed1ab_0",
   "backports.functools_lru_cache 1.6.5 pyhd8ed1ab_0",
   "urllib3 1.26.15 pyhd8ed1ab_0",
   "libmamba 1.4.2 hcea66bb_0",
   "conda-package-handling 2.2.0 pyh38be061_0",
   "traitlets 5.9.0 pyhd8ed1ab_0",
   "pybind11-abi 4 hd8ed1ab_3",
   "more-itertools 10.1.0 pyhd8ed1ab_0",
   "jsonschema-specifications 2023.7.1 pyhd8ed1ab_0",
   "chardet 5.1.0 py310hff52083_0",
   "fmt 9.1.0 h924138e_0",
   "pytz 2023.3 pyhd8ed1ab_0",
   "pillow 10.0.0 py310h582fbeb_0",
   "glob2 0.7 py_0",
   "c-ares 1.19.1 hd590300_0",
   "libedit 3.1.20191231 he28a2e2_2",
   "yaml 0.2.5 h7f98852_2",
   "openjpeg 2.5.0 hfec8fc6_2",
   "attrs 23.1.0 pyh71513ae_1",
   "xorg-libxdmcp 1.1.3 h7f98852_0",
   "psutil 5.9.5 py310h1fa729e_0",
   "wheel 0.41.1 pyhd8ed1ab_0",
   "tomli 2.0.1 pyhd8ed1ab_0",
   "libzlib 1.2.13 hd590300_5",
   "zstd 1.5.2 hfc55251_7",
   "python_abi 3.10 3_cp310",
   "libxcb 1.15 h0b41bf4_0",
   "markupsafe 2.1.3 py310h2372a71_0",
   "yaml-cpp 0.7.0 h27087fc_2",
   "python 3.10.12 hd12c33a_0_cpython",
   "defusedxml 0.7.1 pyhd8ed1ab_0",
   "pkgutil-resolve-name 1.3.10 pyhd8ed1ab_0",
   "ruamel_yaml 0.15.80 py310h5764c6d_1008",
   "pkginfo 1.9.6 pyhd8ed1ab_0",
   "setuptools 68.0.0 pyhd8ed1ab_0",
   "libtiff 4.5.1 h8b53f26_0",
   "json5 0.9.14 pyhd8ed1ab_0",
   "python-dateutil 2.8.2 pyhd8ed1ab_0",
   "cryptography 41.0.3 py310h75e40e8_0",
   "lcms2 2.15 haa2dc70_1",
   "libiconv 1.17 h166bdaf_0",
   "su-exec 0.2 h166bdaf_1003",
   "charset-normalizer 3.2.0 pyhd8ed1ab_0",
   "gettext 0.21.1 h27087fc_0",
   "libsolv 0.7.24 hfc55251_1",
   "libcurl 8.2.1 hca28451_0",
   "keyutils 1.6.1 h166bdaf_0",
   "patch 2.7.6 h7f98852_1002",
   "lz4-c 1.9.4 hcb278e6_0",
   "lerc 4.0.0 h27087fc_0",
   "tini 0.19.0 h166bdaf_1",
   "typing_extensions 4.7.1 pyha770c72_0",
   "libgcc-ng 13.1.0 he5830b7_0",
   "beautifulsoup4 4.12.2 pyha770c72_0",
   "joblib 1.3.2 pyhd8ed1ab_0",
   "curl 8.2.1 hca28451_0",
   "watchgod 0.8.2 pyhd8ed1ab_0",
   "cffi 1.15.1 py310h255011f_3",
   "click 8.1.6 unix_pyh707e725_0",
   "ripgrep 13.0.0 h2f28480_2",
   "libnsl 2.0.0 h7f98852_0",
   "perl 5.32.1 4_hd590300_perl5",
   "markdown-it-py 3.0.0 pyhd8ed1ab_0",
   "py-lief 0.12.3 py310hd8f1fbe_0",
   "conda-index 0.2.3 pyhd8ed1ab_0",
   "nbformat 5.9.2 pyhd8ed1ab_0",
   "_libgcc_mutex 0.1 conda_forge",
   "reproc-cpp 14.2.4 hcb278e6_0",
   "libuuid 2.38.1 h0b41bf4_0",
   "libnghttp2 1.52.0 h61bc06f_0",
   "libwebp-base 1.3.1 hd590300_0",
   "boa 0.15.1 pyhd8ed1ab_0",
   "ca-certificates 2023.7.22 hbcca054_0",
   "liblief 0.12.3 h27087fc_0",
   "pysocks 1.7.1 pyha2e5f31_6",
   "openssl 3.1.2 hd590300_0",
   "backports 1.0 pyhd8ed1ab_3",
   "libpng 1.6.39 h753d276_0",
   "rpds-py 0.9.2 py310hcb5633a_0",
   "icu 72.1 hcb278e6_0",
   "pcre2 10.40 hc3806b6_0",
   "reproc 14.2.4 h0b41bf4_0",
   "zstandard 0.19.0 py310h1275a96_2",
   "libev 4.33 h516909a_1",
   "xorg-libxau 1.0.11 hd590300_0",
   "libarchive 3.6.2 h039dbb9_1",
   "tqdm 4.66.1 pyhd8ed1ab_0",
   "soupsieve 2.3.2.post1 pyhd8ed1ab_0",
   "conda-package-streaming 0.9.0 pyhd8ed1ab_0",
   "wcwidth 0.2.6 pyhd8ed1ab_0",
   "jinja2 3.1.2 pyhd8ed1ab_1",
   "conda 23.3.1 py310hff52083_0",
   "sniffio 1.3.0 pyhd8ed1ab_0",
   "mamba 1.4.2 py310h51d5547_0",
   "pthread-stubs 0.4 h36c2ea0_1001",
   "ruamel.yaml.clib 0.2.7 py310h1fa729e_1",
   "anaconda-client 1.12.0 pyhd8ed1ab_1",
   "brotlipy 0.7.0 py310h5764c6d_1005",
   "prompt_toolkit 3.0.39 hd8ed1ab_0",
   "pluggy 1.2.0 pyhd8ed1ab_0",
   "rich 13.5.1 pyhd8ed1ab_0",
   "python-libarchive-c 5.0 py310hff52083_1",
   "certifi 2023.7.22 pyhd8ed1ab_0",
   "platformdirs 3.10.0 pyhd8ed1ab_0",
   "dataclasses 0.8 pyhc8e2a94_3",
   "pyyaml 6.0 py310h5764c6d_5",
   "pycosat 0.6.4 py310h5764c6d_1",
   "clyent 1.2.2 py_1",
   "idna 3.4 pyhd8ed1ab_0",
   "ncurses 6.4 hcb278e6_0",
   "requests 2.31.0 pyhd8ed1ab_0",
   "python-fastjsonschema 2.18.0 pyhd8ed1ab_0",
   "pyopenssl 23.2.0 pyhd8ed1ab_1",
   "readline 8.2 h8228510_1",
   "conda-forge-ci-setup 3.32.5 py310h7a2d8a0_100",
   "jq 1.6 h36c2ea0_1000",
   "oras-py 0.1.14 pyhd8ed1ab_0",
   "conda-env 2.6.0 1",
   "conda-forge-metadata 0.5.2 pyhd8ed1ab_0",
   "oniguruma 6.9.8 h166bdaf_0",
   "conda-oci-mirror 0.1.0 pyhd8ed1ab_0",
   "shyaml 0.6.2 pyhd3deb0d_0"
  ],
  "summary": "A PyTorch Lightning extension that enhances model experimentation with flexible fine-tuning schedules.",
  "tags": []
 },
 "conda_build_config": {
  "CI": "azure",
  "c_compiler": "gcc",
  "cdt_name": "cos6",
  "channel_sources": "conda-forge",
  "channel_targets": "conda-forge main",
  "cpu_optimization_target": "nocona",
  "cran_mirror": "https://cran.r-project.org",
  "cxx_compiler": "gxx",
  "docker_image": "quay.io/condaforge/linux-anvil-cos7-x86_64",
  "extend_keys": [
   "ignore_build_only_deps",
   "pin_run_as_build",
   "ignore_version",
   "extend_keys"
  ],
  "fortran_compiler": "gfortran",
  "ignore_build_only_deps": [
   "python",
   "numpy"
  ],
  "lua": "5",
  "numpy": "1.22",
  "perl": "5.26.2",
  "pin_run_as_build": {
   "python": {
    "max_pin": "x.x",
    "min_pin": "x.x"
   },
   "r-base": {
    "max_pin": "x.x",
    "min_pin": "x.x"
   }
  },
  "python": "3.10",
  "r_base": "3.5",
  "target_platform": "linux-64"
 },
 "conda_pkg_format": "2",
 "files": [
  "site-packages/finetuning_scheduler-0.4.1.dist-info/INSTALLER",
  "site-packages/finetuning_scheduler-0.4.1.dist-info/LICENSE",
  "site-packages/finetuning_scheduler-0.4.1.dist-info/METADATA",
  "site-packages/finetuning_scheduler-0.4.1.dist-info/RECORD",
  "site-packages/finetuning_scheduler-0.4.1.dist-info/REQUESTED",
  "site-packages/finetuning_scheduler-0.4.1.dist-info/WHEEL",
  "site-packages/finetuning_scheduler-0.4.1.dist-info/direct_url.json",
  "site-packages/finetuning_scheduler/__about__.py",
  "site-packages/finetuning_scheduler/__init__.py",
  "site-packages/finetuning_scheduler/fts.py",
  "site-packages/finetuning_scheduler/fts_supporters.py",
  "site-packages/finetuning_scheduler/py.typed",
  "site-packages/finetuning_scheduler/setup_tools.py",
  "site-packages/finetuning_scheduler/strategy_adapters/__init__.py",
  "site-packages/finetuning_scheduler/strategy_adapters/base.py",
  "site-packages/finetuning_scheduler/strategy_adapters/fsdp.py",
  "site-packages/fts_examples/__init__.py",
  "site-packages/fts_examples/cli_experiment_utils.py",
  "site-packages/fts_examples/config/RteBoolqModule_ft_schedule_deberta_base.yaml",
  "site-packages/fts_examples/config/RteBoolqModule_ft_schedule_deberta_base_fsdp.yaml",
  "site-packages/fts_examples/config/advanced/fsdp/fts_ddp_fsdp_baseline_profile.yaml",
  "site-packages/fts_examples/config/advanced/fsdp/fts_fsdp_awp_overrides_offload_profile.yaml",
  "site-packages/fts_examples/config/advanced/fsdp/fts_fsdp_awp_overrides_profile.yaml",
  "site-packages/fts_examples/config/advanced/reinit_lr/explicit_reinit_lr.yaml",
  "site-packages/fts_examples/config/advanced/reinit_lr/fts_explicit_reinit_lr.yaml",
  "site-packages/fts_examples/config/advanced/reinit_lr/fts_implicit_reinit_lr.yaml",
  "site-packages/fts_examples/config/fts_defaults.yaml",
  "site-packages/fts_examples/config/fts_explicit.yaml",
  "site-packages/fts_examples/config/fts_implicit.yaml",
  "site-packages/fts_examples/config/nofts_baseline.yaml",
  "site-packages/fts_examples/fts_fsdp_superglue.py",
  "site-packages/fts_examples/fts_superglue.py",
  "site-packages/fts_examples/patched_adamw.py"
 ],
 "index": {
  "arch": null,
  "build": "pyhd8ed1ab_0",
  "build_number": 0,
  "depends": [
   "python >=3.7,<3.11",
   "pytorch >=1.10.0,<2.0.0",
   "pytorch-lightning >=1.9.0,<=1.9.4"
  ],
  "license": "Apache-2.0",
  "name": "finetuning-scheduler",
  "noarch": "python",
  "platform": null,
  "subdir": "noarch",
  "timestamp": 1692212107948,
  "version": "0.4.1"
 },
 "metadata_version": 1,
 "name": "finetuning-scheduler",
 "raw_recipe": "{% set name = \"finetuning-scheduler\" %}\n{% set version = \"0.4.1\" %}\n\npackage:\n  name: {{ name|lower }}\n  version: {{ version }}\n\nsource:\n  #url: https://pypi.io/packages/source/{{ name[0] }}/{{ name }}/finetuning-scheduler-{{ version }}.tar.gz\n  url: https://github.com/speediedan/finetuning-scheduler/releases/download/v{{ version }}/finetuning-scheduler-{{ version }}.tar.gz\n  sha256: 131a514b91de0b03467da3c7144a9b6c7a016d6052f02d80736fc13db44fe8fe\n\nbuild:\n  script_env:\n    - PACKAGE_NAME=pytorch\n  number: 0\n  noarch: python\n  script: {{ PYTHON }} -m pip install . -vv\n\nrequirements:\n  host:\n    - pip\n    - python >=3.7, <3.11 # TODO: this upper bound is because of PyTorch issue with dataclass\n  run:\n    - python >=3.7, <3.11 # TODO: this upper bound is because of PyTorch issue with dataclass\n    - pytorch-lightning >=1.9.0, <=1.9.4\n    - pytorch >=1.10.0, <2.0.0\n\ntest:\n  imports:\n    - finetuning_scheduler\n  requires:\n    - pip\n\nabout:\n  home: https://github.com/speediedan/finetuning-scheduler\n  summary: A PyTorch Lightning extension that enhances model experimentation with flexible fine-tuning schedules.\n  license: Apache-2.0\n  license_file: LICENSE\n  doc_url: https://finetuning-scheduler.readthedocs.io/en/stable/\n  dev_url: https://github.com/speediedan/finetuning-scheduler\n  description: |\n    The FinetuningScheduler callback accelerates and enhances foundational model experimentation with flexible fine-tuning\n    schedules. Training with the FinetuningScheduler callback is simple and confers a host of benefits:\n\n    - it dramatically increases fine-tuning flexibility\n    - expedites and facilitates exploration of model tuning dynamics\n    - enables marginal performance improvements of finetuned models\n\n    Fundamentally, the FinetuningScheduler callback enables multi-phase, scheduled fine-tuning of foundational models.\n    Gradual unfreezing (i.e. thawing) can help maximize foundational model knowledge retention while allowing (typically\n    upper layers of) the model to optimally adapt to new tasks during transfer learning.\n\n    FinetuningScheduler orchestrates the gradual unfreezing of models via a fine-tuning schedule that is either implicitly\n    generated (the default) or explicitly provided by the user (more computationally efficient). Fine-tuning phase\n    transitions are driven by FTSEarlyStopping criteria (a multi-phase extension of EarlyStopping), user-specified epoch\n    transitions or a composition of the two (the default mode). A FinetuningScheduler training session completes when the\n    final phase of the schedule has its stopping criteria met.\n\n    Documentation\n    -------------\n    - https://finetuning-scheduler.readthedocs.io/en/stable/\n    - https://finetuning-scheduler.readthedocs.io/en/latest/\n\nextra:\n  recipe-maintainers:\n    - speediedan\n",
 "rendered_recipe": {
  "about": {
   "description": "The FinetuningScheduler callback accelerates and enhances foundational model experimentation with flexible fine-tuning\nschedules. Training with the FinetuningScheduler callback is simple and confers a host of benefits:\n\n- it dramatically increases fine-tuning flexibility\n- expedites and facilitates exploration of model tuning dynamics\n- enables marginal performance improvements of finetuned models\n\nFundamentally, the FinetuningScheduler callback enables multi-phase, scheduled fine-tuning of foundational models.\nGradual unfreezing (i.e. thawing) can help maximize foundational model knowledge retention while allowing (typically\nupper layers of) the model to optimally adapt to new tasks during transfer learning.\n\nFinetuningScheduler orchestrates the gradual unfreezing of models via a fine-tuning schedule that is either implicitly\ngenerated (the default) or explicitly provided by the user (more computationally efficient). Fine-tuning phase\ntransitions are driven by FTSEarlyStopping criteria (a multi-phase extension of EarlyStopping), user-specified epoch\ntransitions or a composition of the two (the default mode). A FinetuningScheduler training session completes when the\nfinal phase of the schedule has its stopping criteria met.\n\nDocumentation\n-------------\n- https://finetuning-scheduler.readthedocs.io/en/stable/\n- https://finetuning-scheduler.readthedocs.io/en/latest/\n",
   "dev_url": "https://github.com/speediedan/finetuning-scheduler",
   "doc_url": "https://finetuning-scheduler.readthedocs.io/en/stable/",
   "home": "https://github.com/speediedan/finetuning-scheduler",
   "license": "Apache-2.0",
   "license_file": "LICENSE",
   "summary": "A PyTorch Lightning extension that enhances model experimentation with flexible fine-tuning schedules."
  },
  "build": {
   "noarch": "python",
   "number": "0",
   "script": "/home/conda/feedstock_root/build_artifacts/finetuning-scheduler_1692211942230/_h_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_/bin/python -m pip install . -vv",
   "script_env": [
    "PACKAGE_NAME=pytorch"
   ],
   "string": "pyhd8ed1ab_0"
  },
  "extra": {
   "copy_test_source_files": true,
   "final": true,
   "recipe-maintainers": [
    "speediedan"
   ]
  },
  "package": {
   "name": "finetuning-scheduler",
   "version": "0.4.1"
  },
  "requirements": {
   "host": [
    "_libgcc_mutex 0.1 conda_forge",
    "_openmp_mutex 4.5 2_gnu",
    "bzip2 1.0.8 h7f98852_4",
    "ca-certificates 2023.7.22 hbcca054_0",
    "ld_impl_linux-64 2.40 h41732ed_0",
    "libffi 3.4.2 h7f98852_5",
    "libgcc-ng 13.1.0 he5830b7_0",
    "libgomp 13.1.0 he5830b7_0",
    "libnsl 2.0.0 h7f98852_0",
    "libsqlite 3.42.0 h2797004_0",
    "libuuid 2.38.1 h0b41bf4_0",
    "libzlib 1.2.13 hd590300_5",
    "ncurses 6.4 hcb278e6_0",
    "openssl 3.1.2 hd590300_0",
    "pip 23.2.1 pyhd8ed1ab_0",
    "python 3.10.12 hd12c33a_0_cpython",
    "readline 8.2 h8228510_1",
    "setuptools 68.0.0 pyhd8ed1ab_0",
    "tk 8.6.12 h27826a3_0",
    "tzdata 2023c h71feb2d_0",
    "wheel 0.41.1 pyhd8ed1ab_0",
    "xz 5.2.6 h166bdaf_0"
   ],
   "run": [
    "python >=3.7, <3.11",
    "pytorch >=1.10.0, <2.0.0",
    "pytorch-lightning >=1.9.0, <=1.9.4"
   ]
  },
  "source": {
   "sha256": "131a514b91de0b03467da3c7144a9b6c7a016d6052f02d80736fc13db44fe8fe",
   "url": "https://github.com/speediedan/finetuning-scheduler/releases/download/v0.4.1/finetuning-scheduler-0.4.1.tar.gz"
  },
  "test": {
   "imports": [
    "finetuning_scheduler"
   ],
   "requires": [
    "pip"
   ]
  }
 },
 "version": "0.4.1"
}